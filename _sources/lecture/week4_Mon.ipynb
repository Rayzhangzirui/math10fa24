{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214442b9",
   "metadata": {},
   "source": [
    "# Lecture Week 4 Mon 10/21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6537069-e84a-40b0-bdb1-e2e107890889",
   "metadata": {},
   "source": [
    "## **Problem**\n",
    "Given the *training dataset* $x_i\\in\\mathbb{R}$, $y_i\\in\\mathbb{R}$, $i= 1,2,..., N$, we want to find the linear function \n",
    "\n",
    "$$ y\\approx f(x) = wx + b $$ \n",
    "\n",
    "that fits the relations between $x_i$ and $y_i$. So that given any $x$, we can make the prediction \n",
    "\n",
    "$$ \\hat{y} = f(x) = w x+b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c1332-f888-4dea-8a0c-d7bcac990ccd",
   "metadata": {},
   "source": [
    "## Loss Function and Optimization\n",
    "\n",
    "With the training dataset, define the loss function $L(w,b)$ of parameter $w$ and $b$, which is also called **mean squared error** (MSE) \n",
    "\n",
    "$$L(w,b)=\\frac{1}{N}\\sum_{i=1}^N\\big(\\hat{y}^{(i)}-y_i\\big)^2=\\frac{1}{N}\\sum_{i=1}^N\\big((wx_i+b)-y_i\\big)^2,$$\n",
    "\n",
    "where $\\hat{y}^{(i)}$ denotes the predicted value of y at $x_i$, i.e. $\\hat{y}^{(i)} = wx_i+b$.\n",
    "\n",
    "Our goal is to find the optimal $w$ and $b$ that minimize the loss function $L(w,b)$, i.e.\n",
    "\n",
    "$$\\min_{w,b} L(w,b)$$\n",
    "\n",
    "\n",
    "This is a function of $w$ and $b$, and we can analytically solve $\\partial_{w}L = \\partial_{b}L =0$, and yields\n",
    "\n",
    "$$w^* = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{X})(y_i - \\bar{Y})}{\\sum_{i=1}^{N} (x_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}$$\n",
    "\n",
    "\n",
    "$$b^* = \\bar{Y}  - w^*\\bar{X}$$\n",
    "\n",
    "where $\\bar{X}$ and $\\bar{Y}$ are the mean of $x$ and of $y$, and $\\text{Cov}(X,Y)$ denotes the estimated covariance (or called sample covariance) between $X$ and $Y$, $\\text{Var}(Y)$ denotes the sample variance of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1113ed-c95e-4c0b-8199-e4fb59683196",
   "metadata": {},
   "source": [
    "### What is the best constant model?\n",
    "\n",
    "What is the best constant $b$ that minimizes the loss function $L(b)$?\n",
    "\n",
    "$$L(b)=\\frac{1}{N}\\sum_{i=1}^N\\big(b-y_i\\big)^2$$\n",
    "\n",
    "We can analytically solve $\\partial_{b}L = 0$, and yields\n",
    "\n",
    "$$b^* = \\bar{y}$$\n",
    "\n",
    "where $\\bar{y}$ is the mean of $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f849ec0-670f-4e0f-9966-608000102ee7",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "- MSE: The smaller MSE indicates better performance. \n",
    "\n",
    "MSE depends on the unit. For example, if we replace y[m] by y[mm], than MSE will be $1000^2$ times the original MSE.\n",
    "\n",
    "\n",
    "- R-Squared: The larger $R^{2}$ (closer to 1) indicates better performance. Compared with MSE, R-squared is **dimensionless**, not dependent on the units of variable. \n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\sum_{i=1}^{N}(y_i-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y_i-\\bar{y})^{2}} = 1 - \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}^{(i)})^{2}}{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\bar{y})^{2}} = 1 - \\frac{\\text{MSE}}{\\text{Var}(Y)}$$\n",
    "\n",
    "Intuitively, MSE is the \"unexplained\" variance, and $R^{2}$ is the proportion of the variance that is explained by the model: \n",
    "If our model is perfect, then MSE is 0, and $R^{2}$ is 1.\n",
    "If we are using a constant model, then our best prediction is the mean of $y$, and MSE is the variance of $y$, and $R^{2}$ is 0; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57946455-583d-457d-9945-0056f109090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([0,1,2,3])\n",
    "y = np.array([0,2,0,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3313b19-57af-4b6f-9fda-0be22818b316",
   "metadata": {},
   "source": [
    "## poll: What is the optimal slope and intercept for the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4877aa0d-ed65-4ed2-8ea6-dc63c175230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLinearRegression:\n",
    "    '''\n",
    "    The single-variable linear regression estimator.\n",
    "    This serves as an example of the regression models from sklearn, with methods fit, predict, and score.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        '''\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # covariance matrix, \n",
    "        # bias = True makes the factor 1/N, otherwise 1/(N-1)\n",
    "        # but it doesn't matter here, since the factor will be cancelled out in the calculation of w\n",
    "        \n",
    "        cov_mat = np.cov(x, y, bias=True)\n",
    "        # cov_mat[0, 1] is the covariance of x and y, and cov_mat[0, 0] is the variance of x\n",
    "\n",
    "        self.w = cov_mat[0, 1] / cov_mat[0, 0]\n",
    "        self.b = np.mean(y) - self.w * np.mean(x)\n",
    "\n",
    "        # :.3f means 3 decimal places\n",
    "        print(f'w = {self.w:.3f}, b = {self.b:.3f}')\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Predict the output values for the input value x, based on trained parameters\n",
    "\n",
    "        '''\n",
    "        ypred = self.w * x + self.b\n",
    "        return ypred\n",
    "\n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        Calculate the R^2 score of the model\n",
    "        '''\n",
    "        mse =  np.mean((y - self.predict(x))**2)\n",
    "        var = np.mean((y - np.mean(y))**2)\n",
    "        Rsquare = 1 - mse / var\n",
    "        return Rsquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45528fa1-2ca6-4ca8-b797-9341ce32208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1.600, b = -0.400\n"
     ]
    }
   ],
   "source": [
    "lm = myLinearRegression()\n",
    "lm.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4afb5be-0b23-4126-8424-a07acbf26ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score = 0.533\n"
     ]
    }
   ],
   "source": [
    "score = lm.score(x, y)\n",
    "print(f'R^2 score = {score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba927c71-3c5e-446a-8303-7ff93f06af50",
   "metadata": {},
   "source": [
    "Use the following command to install scikit-learn\n",
    "\n",
    "`conda install scikit-learn`\n",
    "\n",
    "or\n",
    "\n",
    "`pip install scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e63945-3440-4dab-ae6d-fca2b6774afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1.600, b = -0.400\n",
      "score = 0.533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "# For sklearn, the input x has to be n-by-d, where n is the number of samples and d is the number of features\n",
    "# x.reshape(-1, 1) reshapes the 1D array x to a 2D array with 1 column\n",
    "# \n",
    "\n",
    "reg.fit(x.reshape(-1, 1), y)\n",
    "print(f'w = {reg.coef_[0]:.3f}, b = {reg.intercept_:.3f}')\n",
    "\n",
    "score = reg.score(x.reshape(-1, 1), y)\n",
    "print(f'score = {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d48810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "# side note on numpy.reshape\n",
    "\n",
    "x = np.array([0,1,2,3,4,5])\n",
    "\n",
    "# reshape to 2D array with 2 rows and 3 columns\n",
    "print(x.reshape(2, 3))\n",
    "\n",
    "# if we set the number of rows to -1, numpy will automatically calculate the number of rows\n",
    "print(x.reshape(-1, 3))\n",
    "\n",
    "# reshape to n-by-1 array\n",
    "print(x.reshape(-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
